import pandas as pd
import os
import sys

# Ajout de la racine du projet (backend/) au PATH pour permettre les imports depuis 'src'
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

try:
    from src.augmentation import augment_signal
    from src.features import run_feature_extraction
    from src.acquisition import fetch_lightcurve
    from src.preprocessing import clean_lightcurve
except ImportError as e:
    print(f"‚ùå Erreur d'importation : {e}")
    print("üí° Assurez-vous d'√™tre dans le dossier 'backend' et que le dossier 'src' contient bien __init__.py")
    sys.exit(1)

def find_id_column(df):
    """
    Cherche la colonne qui contient les noms des √©toiles.
    On cherche une colonne qui contient des cha√Ænes de caract√®res (Kepler, TIC, etc.)
    """
    # 1. Candidats prioritaires par nom
    priority_candidates = ['target_id', 'flux__id', 'id', 'target', 'Unnamed: 0']
    for cand in priority_candidates:
        if cand in df.columns:
            # V√©rification sommaire : est-ce que √ßa ressemble √† un ID (pas juste des 0 et 1)
            sample_val = str(df[cand].iloc[0])
            if not sample_val.replace('.','').isdigit() or any(c in sample_val for c in ['K', 'T', 'i', 'c']):
                return cand

    # 2. Recherche par contenu (on cherche la premi√®re colonne non-num√©rique ou contenant du texte)
    for col in df.columns:
        sample_val = str(df[col].iloc[0])
        # Un ID d'exoplan√®te contient souvent des lettres (Kepler, TIC, TOI, Pi...)
        if any(char.isalpha() for char in sample_val):
            return col
            
    return None

def run_augmentation_pipeline():
    """
    G√©n√®re le dataset augment√© √† partir des 123 √©toiles nettoy√©es.
    Chaque √©toile r√©elle g√©n√®re 3 variantes artificielles (Noisy, Deep, Shallow).
    """
    input_file = "data/processed/training_dataset_clean.csv"
    output_file = "data/processed/final_augmented_dataset.csv"
    
    if not os.path.exists(input_file):
        print(f"‚ùå Erreur : {input_file} introuvable. Veuillez d'abord nettoyer vos donn√©es.")
        return

    # Chargement du dataset de base
    df_orig = pd.read_csv(input_file)
    
    # --- D√âTECTION INTELLIGENTE DE LA COLONNE ID ---
    id_col = find_id_column(df_orig)
    
    if id_col is None:
        print("‚ùå ERREUR CRITIQUE : Impossible de trouver la colonne des noms d'√©toiles (ID).")
        print("üí° Votre fichier CSV semble ne contenir que des chiffres (metrics).")
        print("üí° V√©rifiez que vous n'avez pas supprim√© la colonne 'target_id' lors du nettoyage.")
        return

    print(f"üöÄ D√âMARRAGE DE L'AUGMENTATION : {len(df_orig)} cibles r√©elles.")
    print(f"üîç Colonne identifi√©e pour les IDs : '{id_col}'")

    all_data = [df_orig] 

    for idx, row in df_orig.iterrows():
        # Extraction s√©curis√©e de l'ID
        target_id = str(row[id_col])
        label = row['target_label']
        
        # On ignore les lignes sans ID valide ou les valeurs aberrantes
        if not target_id or target_id.lower() in ["nan", "none", "0.0", "0", "1.0", "1"]:
            continue

        print(f"üîÑ [{idx+1}/{len(df_orig)}] Traitement de {target_id}...")
        
        try:
            # 1. D√©tection automatique de la mission
            mission = "TESS" if any(x in target_id for x in ["TIC", "TOI", "Pi ", "LHS", "WASP", "AU ", "GJ ", "HD "]) else "Kepler"
            
            # 2. Acquisition des donn√©es NASA
            lc_raw = fetch_lightcurve(target_id, mission=mission)
            
            if lc_raw is None:
                print(f"   ‚ö†Ô∏è Donn√©es introuvables pour {target_id}.")
                continue
            
            # 3. Pr√©traitement adaptatif
            lc_clean = clean_lightcurve(lc_raw, quality="auto")
            
            # 4. G√©n√©ration des clones
            variations = augment_signal(lc_clean)
            
            # 5. Extraction des caract√©ristiques pour chaque clone
            for suffix, lc_var in variations:
                new_id = f"{target_id}_{suffix}"
                df_var = run_feature_extraction(lc_var, new_id)
                df_var['target_label'] = label
                all_data.append(df_var)
                
            # Sauvegarde de secours r√©guli√®re
            if idx % 5 == 0:
                pd.concat(all_data, ignore_index=True).to_csv(output_file, index=False)
                
        except Exception as e:
            print(f"   ‚ö†Ô∏è √âchec critique pour {target_id} : {e}")
            continue

    # 6. Fusion finale
    print("\n--- üèÅ FINALISATION DU GIGA DATASET ---")
    if len(all_data) > 1:
        final_df = pd.concat(all_data, ignore_index=True).fillna(0)
        final_df.to_csv(output_file, index=False)
        print(f"‚ú® TERMIN√â : Dataset cr√©√© avec {len(final_df)} √©chantillons.")
    else:
        print("‚ùå Aucune donn√©e n'a pu √™tre augment√©e.")

if __name__ == "__main__":
    run_augmentation_pipeline()