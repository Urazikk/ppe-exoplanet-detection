import pandas as pd
import os
import time
from acquisition import fetch_lightcurve
from preprocessing import clean_lightcurve
from features import run_feature_extraction

# --- CATALOGUE DE DONN√âES D'ENTRA√éNEMENT √âLARGI (KEPLER + TESS) ---
# Label 1 : Plan√®tes Confirm√©es
# Label 0 : Faux Positifs ou √âtoiles Seules
CATALOG = [
    # --- CONFIRM√âES KEPLER (Historique) ---
    {"id": "Kepler-10", "label": 1}, {"id": "Kepler-90", "label": 1},
    {"id": "Kepler-1", "label": 1},  {"id": "Kepler-2", "label": 1},
    {"id": "Kepler-8", "label": 1},  {"id": "Kepler-11", "label": 1},
    
    # --- CONFIRM√âES TESS (Rapides et l√©g√®res) ---
    {"id": "Pi Mensae", "label": 1},   # TIC 261136679
    {"id": "TOI-700", "label": 1},     # Syst√®me multi-plan√©taire c√©l√®bre
    {"id": "TOI-270", "label": 1},     # Super-Terres et mini-Neptunes
    {"id": "TOI-175", "label": 1},     # L 98-59
    {"id": "TOI-132", "label": 1},     # Neptune chaude
    {"id": "TOI-1148", "label": 1},    # Saturne chaude
    {"id": "WASP-18", "label": 1},     # Jupiter ultra-chaude (Transit tr√®s profond)
    {"id": "LHS 3844", "label": 1},    # Plan√®te tellurique
    {"id": "AU Mic", "label": 1},      # √âtoile jeune avec plan√®te
    {"id": "GJ 357", "label": 1},      # Syst√®me avec Super-Terre
    {"id": "HD 21749", "label": 1},    # Syst√®me brillant
    
    # --- FAUX POSITIFS / BRUIT (Pour apprendre les erreurs) ---
    {"id": "Kepler-411", "label": 0}, {"id": "Kepler-466", "label": 0},
    {"id": "TIC 278825448", "label": 0}, # Binaire √† √©clipse (Simule un transit)
    {"id": "TIC 238196510", "label": 0}, # Binaire √† √©clipse
    {"id": "Kepler-699", "label": 0}, {"id": "Kepler-707", "label": 0},
    {"id": "Kepler-711", "label": 0}, {"id": "Kepler-715", "label": 0},
    {"id": "Kepler-717", "label": 0}, {"id": "Kepler-719", "label": 0},
]

def build_training_data():
    output_dir = "data/processed/"
    os.makedirs(output_dir, exist_ok=True)
    master_file = os.path.join(output_dir, "training_dataset.csv")
    
    print(f"üöÄ Lancement du pipeline massif sur {len(CATALOG)} cibles.")
    
    if os.path.exists(master_file):
        master_df = pd.read_csv(master_file)
        # On v√©rifie les IDs d√©j√† pr√©sents pour ne pas les refaire
        processed_ids = master_df['flux__id'].unique().tolist() if 'flux__id' in master_df.columns else []
    else:
        master_df = pd.DataFrame()
        processed_ids = []

    for sample in CATALOG:
        target = sample["id"]
        label = sample["label"]
        
        if target in processed_ids:
            print(f"‚è© {target} d√©j√† pr√©sent dans le dataset.")
            continue
            
        print(f"\nüõ∞Ô∏è ANALYSE : {target} (Label: {label})")
        
        try:
            # On laisse fetch_lightcurve d√©cider de la mission automatiquement
            # mais on peut forcer TESS si l'ID commence par 'TIC' ou 'TOI'
            mission = "TESS" if ("TIC" in target or "TOI" in target or "Pi Mensae" in target) else "Kepler"
            author = "SPOC" if mission == "TESS" else "Kepler"
            
            lc_raw = fetch_lightcurve(target, mission=mission, author=author)
            if lc_raw is None: continue
            
            # Pour TESS, on n'a souvent pas besoin de binning (d√©j√† l√©ger)
            quality = "high" if len(lc_raw) < 100000 else "ultra"
            lc_clean = clean_lightcurve(lc_raw, quality=quality).remove_nans()
            
            # Extraction
            df_feat = run_feature_extraction(lc_clean, target)
            df_feat['target_label'] = label
            
            # Concatenation
            if master_df.empty:
                master_df = df_feat
            else:
                master_df = pd.concat([master_df, df_feat], ignore_index=True)
            
            # Sauvegarde √† chaque √©tape
            master_df.to_csv(master_file, index=False)
            print(f"‚úÖ {target} ajout√©.")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur sur {target} : {e}")
            continue

    print(f"\nüèÅ Termin√©. Dataset total : {len(master_df)} exemples.")

if __name__ == "__main__":
    build_training_data()